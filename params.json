{
  "name": "DRAW",
  "tagline": "Deep Recurrent Attentive Writer",
  "body": "# Deep Recurrent Attentive Writer\r\nThis post implements the Deep Recurrent Attentive Writer\r\n\r\nMany people preceded me with fascinating implementations\r\n  * [Am implementation in Torch](https://github.com/vivanov879/draw)\r\n  * [An implementation in Theano](https://github.com/jbornschein/draw)\r\n  * [An implementation in Tensorflow with Pretty Tensor](https://github.com/ikostrikov/TensorFlow-VAE-GAN-DRAW)\r\n  * [A clean implementation in Tensorflow](https://github.com/ericjang/draw)\r\n\r\nThis implementation goes end to end in 250 lines of code. We start at `import MNIST` and end up with these lovely visualizations.\r\n\r\n## Read\r\n![READ_DRAW](https://github.com/RobRomijnders/DRAW/blob/master/canvas/mnist_read.gif?raw=true)\r\n## Write\r\n![WRITE_DRAW](https://github.com/RobRomijnders/DRAW/blob/master/canvas/mnist_write.gif?raw=true)\r\n\r\nThis series of posts, implementing an [Variational autoencoder](https://robromijnders.github.io/VAE/), followed by a [variational recurrent autoencoder](https://robromijnders.github.io/VAE_rec/) and now the DRAW was inspired after [Karol Gregor's lecture at Oxford](https://www.youtube.com/watch?v=P78QYjWh5sM). More recently, he [ties his work together at ICML](https://dl.dropboxusercontent.com/u/16027344/ICML%202015%20Deep%20Learning%20Workshop/Karol%20Gregor%2C%20GOOGLE%20Deepmind.p2g/Default.html) (or use [this](https://www.youtube.com/watch?v=en8UlmZ9w6E) link).\r\n\r\nMoreover if you'd like a better introduction to variational inference, go for this talk by [Diederik Kingma at ICLR](https://www.youtube.com/watch?v=rjZL7aguLAs) or go for [his paper](https://arxiv.org/abs/1312.6114).\r\n\r\nAs always, I am curious to any comments and questions. Reach me at romijndersrob@gmail.com\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}